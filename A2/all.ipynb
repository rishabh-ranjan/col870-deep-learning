{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from ipywidgets import Output\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from tqdm.auto import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, norm_layer, in_channels, out_channels, down_sample=False):\n",
    "        super().__init__()\n",
    "        self.norm_layer = norm_layer\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        if down_sample:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = norm_layer(out_channels)\n",
    "        self.bn2 = norm_layer(out_channels)\n",
    "        if not self.in_channels == self.out_channels:\n",
    "            self.proj = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.in_channels == self.out_channels:\n",
    "            return F.relu(x + self.bn2(self.conv2(F.relu(self.bn1(self.conv1(x))))))\n",
    "        else:\n",
    "            return F.relu(self.proj(x) + self.bn2(self.conv2(F.relu(self.bn1(self.conv1(x))))))\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n=2):\n",
    "        super().__init__()\n",
    "        self.norm_layer = nn.BatchNorm2d\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = self.norm_layer(16)\n",
    "        layer1 = []\n",
    "        for i in range(n):\n",
    "            layer1.append(ResBlock(self.norm_layer, 16, 16))\n",
    "        self.layer1 = nn.ModuleList(layer1)\n",
    "        layer2 = []\n",
    "        for i in range(n):\n",
    "            if i == 0:\n",
    "                layer2.append(ResBlock(self.norm_layer, 16, 32, down_sample=True))\n",
    "            else:\n",
    "                layer2.append(ResBlock(self.norm_layer, 32, 32))\n",
    "        self.layer2 = nn.ModuleList(layer2)\n",
    "        layer3 = []\n",
    "        for i in range(n):\n",
    "            if i == 0:\n",
    "                layer3.append(ResBlock(self.norm_layer, 32, 64, down_sample=True))\n",
    "            else:\n",
    "                layer3.append(ResBlock(self.norm_layer, 64, 64))\n",
    "        self.layer3 = nn.ModuleList(layer3)\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(64, 9) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        for layer in self.layer1:\n",
    "            x = layer(x)\n",
    "        for layer in self.layer2:\n",
    "            x = layer(x)\n",
    "        for layer in self.layer3:\n",
    "            x = layer(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x.flatten(start_dim=1) \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRN(nn.Module):\n",
    "    def __init__(self, n_steps, step_loss=False):\n",
    "        super().__init__()\n",
    "        self.n_steps = n_steps\n",
    "        make_mlp = lambda i: nn.Sequential(\n",
    "                                nn.Linear(i,96), nn.ReLU(),\n",
    "                                nn.Linear(96,96), nn.ReLU(),\n",
    "                                nn.Linear(96,96), nn.ReLU(),\n",
    "                                nn.Linear(96,16)\n",
    "                            )\n",
    "        self.inp_enc = make_mlp(25)\n",
    "        self.msg_enc = make_mlp(32)\n",
    "        self.msg_comb = make_mlp(32)\n",
    "        self.lstm_cell = nn.LSTMCell(16,16)\n",
    "        self.decoder = nn.Linear(16,8)\n",
    "        self.rc = self.get_rc()\n",
    "        self.l, self.r = self.get_lr()\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.step_loss = step_loss\n",
    "        \n",
    "    def get_rc(self):\n",
    "        t = F.one_hot(torch.arange(8, device=device))\n",
    "        rc = torch.cat((t.repeat(8,1), t.repeat(1,8).view(-1,8)), dim=-1)\n",
    "        return rc.float()\n",
    "    \n",
    "    def get_lr(self):\n",
    "        s = set()\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                start = 8*i+j\n",
    "                for x in range(8):\n",
    "                    end = 8*i+x\n",
    "                    s.add((start,end))\n",
    "                    end = 8*x+j\n",
    "                    s.add((start,end))\n",
    "                block_start_x = i//2*2\n",
    "                block_start_y = j//4*4\n",
    "                for x in range(2):\n",
    "                    for y in range(4):\n",
    "                        X, Y = block_start_x + x, block_start_y + y\n",
    "                        end = 8*X + Y\n",
    "                        s.add((start,end))\n",
    "        l, r = zip(*s)\n",
    "        return torch.tensor(l, dtype=torch.long), torch.tensor(r, dtype=torch.long)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        b = X.shape[0]\n",
    "        X = X.view(-1,64,9)\n",
    "        RC = self.rc[None,:,:].to(X.device).expand(b,-1,-1)\n",
    "        X = self.inp_enc(torch.cat((RC, X.float()), dim=-1)).view(-1,16)\n",
    "        H = X\n",
    "        C = torch.zeros_like(H)\n",
    "        self.out = []\n",
    "        for step in range(self.n_steps):\n",
    "            Hv = H.view(-1,64,16)\n",
    "            M = torch.zeros(b,64,64,16, device=H.device)\n",
    "            M[:,self.l,self.r,:] = self.msg_enc(torch.cat((Hv[:,self.l,:], Hv[:,self.r,:]), dim=-1))\n",
    "            XM = self.msg_comb(torch.cat((X, torch.sum(M, dim=-2).view(-1,16)), dim=-1))\n",
    "            H, C = self.lstm_cell(XM, (H, C))\n",
    "            O = self.decoder(H)\n",
    "            if self.step_loss:\n",
    "                self.out.append(O)\n",
    "        return O.view(-1,512)\n",
    "    \n",
    "    def criterion(self, y_true, X_pred):\n",
    "        if self.step_loss:\n",
    "            self.losses = torch.empty(self.n_steps, device=y_true.device)\n",
    "            for step in range(self.n_steps):\n",
    "                self.losses[step] = self.cross_entropy_loss(self.out[step], y_true.view(-1))\n",
    "            return torch.sum(self.losses)\n",
    "        else:\n",
    "            return self.cross_entropy_loss(X_pred.view(-1,8), y_true.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, n_classes):\n",
    "        super().__init__()\n",
    "        make_block = lambda i, o, k, s: nn.Sequential(\n",
    "            nn.ConvTranspose2d(i, o, k, s),\n",
    "            nn.BatchNorm2d(o),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gen = nn.Sequential(\n",
    "            make_block(z_dim+n_classes, 256, 3, 2),\n",
    "            make_block(256, 128, 4, 1),\n",
    "            make_block(128, 64, 3, 2),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.z_dim = z_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, Z, Y):\n",
    "        return self.gen(torch.cat((Z,Y), dim=-1).view(-1,self.z_dim+self.n_classes,1,1))\n",
    "    \n",
    "    def criterion(self, fake_yhat):\n",
    "        return self.bce_loss(fake_yhat, torch.ones_like(fake_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointGenerator(nn.Module):\n",
    "    def __init__(self, resnet, n_steps, z_dim, w):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet\n",
    "        self.rrn = RRN(n_steps, step_loss=False)\n",
    "        self.gen = Generator(z_dim, 8)\n",
    "        self.z_dim = z_dim\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.w = w\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = self.rrn(self.resnet(X).detach().view(-1,576)).view(-1,8)\n",
    "        Z = torch.randn(Y.shape[0], self.z_dim, device=Y.device)\n",
    "        return self.gen(Z, Y)\n",
    "    \n",
    "    def criterion(self, fake_yhat, real_X, fake_X):\n",
    "        return self.gen.criterion(fake_yhat) +  self.w * self.l1_loss(real_X, fake_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        make_block = lambda i, o, k, s: nn.Sequential(\n",
    "            nn.Conv2d(i, o, k, s),\n",
    "            nn.BatchNorm2d(o),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.disc = nn.Sequential(\n",
    "            make_block(1+n_classes,64,4,2),\n",
    "            make_block(64,128,4,2),\n",
    "            nn.Conv2d(128,1,4,2)\n",
    "        )\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.disc(torch.cat((X, Y[:,:,None,None].repeat(1,1,28,28)), dim=1)).view(-1,1)\n",
    "    \n",
    "    def criterion(self, real_yhat, fake_yhat):\n",
    "        return (self.bce_loss(real_yhat, torch.ones_like(real_yhat)) +\n",
    "                self.bce_loss(fake_yhat, torch.zeros_like(fake_yhat)))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sudoku_images(path, total, device, normalize=False):\n",
    "    sudoku_img = torch.empty(total,1,224,224, device=device)\n",
    "    if normalize:\n",
    "        transform = transforms.Compose((\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,),(0.5,))))\n",
    "    else:\n",
    "        transform = transforms.ToTensor()\n",
    "    for i in tqdm(range(total), 'sudoku images'):\n",
    "        sudoku_img[i,0] = transform(Image.open(f'{path}/{i}.png'))\n",
    "    return sudoku_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_X = load_sudoku_images('data/query', 10000, device, normalize=True)\n",
    "# torch.save(query_X, 'data/pt-cache/query_X.pt')\n",
    "query_X = torch.load('data/pt-cache/query_X.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_X = load_sudoku_images('data/target', 10000, device, normalize=True)\n",
    "# torch.save(target_X, 'data/pt-cache/target_X.pt')\n",
    "target_X = torch.load('data/pt-cache/target_X.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sudoku_img(sudoku_img):\n",
    "    return torch.stack(torch.split(\n",
    "        torch.stack(torch.split(sudoku_img, [28]*8, dim=-2), dim=-3),\n",
    "        [28]*8, dim=-1), dim=-3).view(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_sudoku(img):\n",
    "    return utils.make_grid(img, nrow=8, padding=0).view(-1,1,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_images(img, nrow):\n",
    "    plt.imshow(utils.make_grid(((img+1)/2).detach().cpu(), nrow=nrow, padding=0).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet().to(device)\n",
    "resnet.load_state_dict(torch.load('data/pt-cache/resnet.pt'))\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad=False\n",
    "gen = JointGenerator(resnet=resnet, n_steps=16, z_dim=64, w=1).to(device)\n",
    "disc = Discriminator(n_classes=8).to(device)\n",
    "gen_opt = optim.Adam(gen.parameters())\n",
    "disc_opt = optim.Adam(disc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "gen_losses = []\n",
    "disc_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edad3262884416c9295cc788b1730a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1516817efcd4c2cb13e3cd9740b87e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='batches'), FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9920ce39ae40a0a232347b6569a150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='batches'), FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = DataLoader(TensorDataset(query_X, target_X), batch_size=32, shuffle=True)\n",
    "gen_opt = optim.Adam(gen.parameters(), lr=1e-4)\n",
    "disc_opt = optim.Adam(disc.parameters(), lr=1e-4)\n",
    "show_times = False\n",
    "plt_out = Output()\n",
    "display.display(plt_out)\n",
    "\n",
    "while True:\n",
    "    for X, real_X in tqdm(loader, 'batches'):\n",
    "        ctr += 1\n",
    "        \n",
    "        tic = time.time()\n",
    "        X = split_sudoku_img(X.to(device))\n",
    "        real_X = split_sudoku_img(real_X.to(device))\n",
    "        toc = time.time()\n",
    "        if show_times:\n",
    "            print('move+split:\\t', toc-tic)\n",
    "        \n",
    "        tic = time.time()\n",
    "        fake_X = gen(X)\n",
    "        toc = time.time()\n",
    "        if show_times:\n",
    "            print('gen:\\t\\t', toc-tic)\n",
    "        \n",
    "        tic = time.time()\n",
    "        y = resnet(real_X)[...,1:]\n",
    "        toc = time.time()\n",
    "        if show_times:\n",
    "            print('resnet:\\t\\t', toc-tic)\n",
    "        \n",
    "        tic = time.time()\n",
    "        real_yhat = disc(real_X, y.detach())\n",
    "        fake_yhat = disc(fake_X.detach(), y.detach())\n",
    "        toc = time.time()\n",
    "        if show_times:\n",
    "            print('disc:\\t\\t', toc-tic)\n",
    "        \n",
    "        tic = time.time()\n",
    "        disc_loss = disc.criterion(real_yhat, fake_yhat)\n",
    "        disc_losses.append(disc_loss.item())\n",
    "        disc_opt.zero_grad()\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        nn.utils.clip_grad_norm_(disc.parameters(), 1)\n",
    "        disc_opt.step()\n",
    "        toc = time.time()\n",
    "        if show_times:\n",
    "            print('disc back:\\t', toc-tic)\n",
    "        \n",
    "        tic = time.time()\n",
    "        fake_yhat = disc(fake_X, y)\n",
    "        toc = time.time()\n",
    "        if show_times:\n",
    "            print('disc:\\t\\t', toc-tic)\n",
    "        \n",
    "        tic = time.time()\n",
    "        gen_loss = gen.criterion(fake_yhat, real_X, fake_X)\n",
    "        gen_losses.append(gen_loss.item())\n",
    "        gen_opt.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(gen.parameters(), 1)\n",
    "        gen_opt.step()\n",
    "        toc = time.time()\n",
    "        if show_times:\n",
    "            print('gen back:\\t', toc-tic)\n",
    "            print('---')\n",
    "\n",
    "        if ctr % 10 == 0:\n",
    "#             print('gen:',gen_losses[-1],'\\t','disc:',disc_losses[-1])\n",
    "\n",
    "            with plt_out:\n",
    "                plt.figure(figsize=(15,5))\n",
    "                plt.subplot(131)\n",
    "                viz_images(X[:64,:,:,:], nrow=8)\n",
    "                plt.subplot(132)\n",
    "                viz_images(real_X[:64,:,:,:], nrow=8)\n",
    "                plt.subplot(133)\n",
    "                viz_images(fake_X[:64,:,:,:], nrow=8)\n",
    "                plt.show()\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.plot(gen_losses, label='gen')\n",
    "                plt.plot(disc_losses, label='disc')\n",
    "                plt.legend()\n",
    "                plt.xlabel('batches')\n",
    "                plt.ylabel('loss')\n",
    "                plt.title(f'Loss Curve (batch_size={loader.batch_size})')\n",
    "                plt.show()\n",
    "                display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
