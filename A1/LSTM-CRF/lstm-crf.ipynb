{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import IPython as ipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "import itertools as it\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb(path, total=None):\n",
    "    toks = []\n",
    "    embs = []\n",
    "    with open(path, 'r') as f:\n",
    "        for l in tqdm(f, path, total=total):\n",
    "            tok, *emb = l.strip().split()\n",
    "            emb = [float(x) for x in emb]\n",
    "            toks.append(tok)\n",
    "            embs.append(emb)\n",
    "    assert('PAD_TOK' not in toks and 'UNK_TOK' not in toks)\n",
    "    toks += ['PAD_TOK', 'UNK_TOK']\n",
    "    embs += [[0.]*len(emb), [0.]*len(emb)]\n",
    "    tok_to_id = dict(zip(toks, it.count()))\n",
    "    emb = torch.tensor(embs)\n",
    "    return tok_to_id, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok_to_id, glv_emb = load_emb('data/glove/glove.6B.100d.txt', int(4e5))\n",
    "#torch.save((tok_to_id, glv_emb), 'data/pt-cache/tok_to_id__glv_emb.pt')\n",
    "tok_to_id, glv_emb = torch.load('data/pt-cache/tok_to_id__glv_emb.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chars(path, total=None):\n",
    "    chars = set()\n",
    "    with open(path, 'r') as f:\n",
    "        for l in tqdm(f, path, total=total):\n",
    "            try:\n",
    "                for c in l.strip().split()[2]:\n",
    "                    chars.add(c)\n",
    "            except:\n",
    "                pass\n",
    "    assert('PAD_CHR' not in chars and 'UNK_CHR' not in chars)\n",
    "    chars.add('PAD_CHR')\n",
    "    chars.add('UNK_CHR')\n",
    "    return dict(zip(chars, it.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chr_to_id = load_chars('data/ner-gmb/train+dev.txt')\n",
    "#torch.save(chr_to_id, 'data/pt-cache/chr_to_id.pt')\n",
    "chr_to_id = torch.load('data/pt-cache/chr_to_id.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes(path, total=None):\n",
    "    id_to_lbl = set()\n",
    "    with open(path, 'r') as f:\n",
    "        for l in tqdm(f, path, total=total):\n",
    "            try:\n",
    "                id_to_lbl.add(l.strip().split()[3])\n",
    "            except:\n",
    "                pass\n",
    "    assert('PAD_LBL' not in id_to_lbl)\n",
    "    id_to_lbl.add('PAD_LBL')\n",
    "    id_to_lbl = list(id_to_lbl)\n",
    "    lbl_to_id = {k:v for v, k in enumerate(id_to_lbl)}\n",
    "    return lbl_to_id, id_to_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lbl_to_id, id_to_lbl = load_classes('data/ner-gmb/train.txt')\n",
    "#torch.save((lbl_to_id, id_to_lbl), 'data/pt-cache/lbl_to_id__id_to_lbl')\n",
    "lbl_to_id, id_to_lbl = torch.load('data/pt-cache/lbl_to_id__id_to_lbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, tok_to_id, lbl_to_id, chr_to_id, seq_len=128, word_len=64):\n",
    "    with open(path, 'r') as f:\n",
    "        seqs = f.read().split('\\n\\n')\n",
    "        seqs.pop()\n",
    "        seqs[0] = seqs[0][1:]\n",
    "    X = tok_to_id['PAD_TOK'] * torch.ones((len(seqs), seq_len), dtype=torch.long)\n",
    "    Y = lbl_to_id['PAD_LBL'] * torch.ones((len(seqs), seq_len), dtype=torch.long)\n",
    "    W = chr_to_id['PAD_CHR'] * torch.ones((len(seqs), seq_len, word_len), dtype=torch.long)\n",
    "    for i, seq in enumerate(tqdm(seqs, 'sequences')):\n",
    "        for j, l in enumerate(seq.split('\\n')):\n",
    "            assert(j < seq_len)\n",
    "            tok, _, wrd, lbl = l.split(' ')\n",
    "            try:\n",
    "                X[i,j] = tok_to_id[tok]\n",
    "            except KeyError:\n",
    "                X[i,j] = tok_to_id['UNK_TOK']\n",
    "                \n",
    "            for k, ch in enumerate(wrd):\n",
    "                try:\n",
    "                    W[i,j,k] = chr_to_id[ch]\n",
    "                except KeyError:\n",
    "                    W[i,j,k] = chr_to_id['UNK_CHR']\n",
    "                    \n",
    "            Y[i,j] = lbl_to_id[lbl]\n",
    "    return X, Y, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X, train_Y, train_W = load_data('data/ner-gmb/train.txt', tok_to_id, lbl_to_id, chr_to_id)\n",
    "#torch.save((train_X, train_Y, train_W), 'data/pt-cache/train_X__train_Y__train_W.pt')\n",
    "train_X, train_Y, train_W = torch.load('data/pt-cache/train_X__train_Y__train_W.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_X, dev_Y, dev_W = load_data('data/ner-gmb/dev.txt', tok_to_id, lbl_to_id, chr_to_id)\n",
    "#torch.save((dev_X, dev_Y, dev_W), 'data/pt-cache/dev_X__dev_Y__dev_W.pt')\n",
    "dev_X, dev_Y, dev_W = torch.load('data/pt-cache/dev_X__dev_Y__dev_W.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_X, test_Y, test_W = load_data('data/ner-gmb/test.txt', tok_to_id, lbl_to_id, chr_to_id)\n",
    "#torch.save((test_X, test_Y, test_W), 'data/pt-cache/test_X__test_Y__test_W.pt')\n",
    "test_X, test_Y, test_W = torch.load('data/pt-cache/test_X__test_Y__test_W.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(nn.Module):\n",
    "    def __init__(self, embed_model, seq_tag_model, pad_lbl_id):\n",
    "        super().__init__()\n",
    "        self.embed_model = embed_model\n",
    "        self.seq_tag_model = seq_tag_model\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=pad_lbl_id)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.seq_tag_model(self.embed_model(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            return torch.argmax(self(X), dim=-1)\n",
    "        \n",
    "    def criterion(self, Y, Y_hat):\n",
    "        return self.cross_entropy_loss(Y_hat.transpose(1,2), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqTagModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.h0 = nn.Parameter(torch.zeros(2, hidden_size))\n",
    "        self.c0 = nn.Parameter(torch.zeros(2, hidden_size))\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(2*hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        D = self.dropout(X)\n",
    "        H, _ = self.lstm(D, (self.h0.expand(D.shape[0],-1,-1), self.c0.expand(D.shape[0],-1,-1)))\n",
    "        return self.linear(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChrEmbModel(nn.Module):\n",
    "    def __init__(self, n_embs, pad_chr_id, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_embs, input_size, padding_idx=pad_chr_id)\n",
    "        self.h0 = nn.Parameter(torch.zeros(2, hidden_size))\n",
    "        self.c0 = nn.Parameter(torch.zeros(2, hidden_size))\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, W):\n",
    "        X = W.reshape(-1,W.shape[-1])\n",
    "        E = self.embedding(X)\n",
    "        _, (H, _) = self.lstm(E, (self.h0.expand(E.shape[0],-1,-1), self.c0.expand(E.shape[0],-1,-1)))\n",
    "        return H.reshape(*W.shape,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChrTokEmbModel(nn.Module):\n",
    "    def __init__(self, chr_emb_model, tok_emb_model):\n",
    "        super().__init__()\n",
    "        self.chr_emb_model = chr_emb_model\n",
    "        self.tok_emb_model = tok_emb_model\n",
    "    \n",
    "    def forward(self, W, X):\n",
    "        return torch.cat((self.chr_emb_model(W), self.tok_emb_model(X)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NERModel(100, 100, len(lbl_to_id)-1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
